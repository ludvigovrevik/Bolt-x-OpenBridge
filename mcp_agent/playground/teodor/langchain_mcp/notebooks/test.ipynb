{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20daa0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install smithery langchain-mcp-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2531e0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "SMITHERY_API_KEY = os.getenv(\"SMITHERY_API_KEY\")\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "\n",
    "print(python_repl.run(\"print('hello world')\"))\n",
    "\n",
    "# install smithery, langchain_mcp_tools\n",
    "import smithery\n",
    "import mcp\n",
    "from langchain_mcp_tools import convert_mcp_to_langchain_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# MCP To LangChain Tools Conversion Utility, install langchain_mcp_tools\n",
    "# https://pypi.org/project/langchain-mcp-tools/ \n",
    "\n",
    "# login to Smithery: https://smithery.ai/\n",
    "# create a API Key\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55626cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "struct_llm = llm.with_structured_output(list)\n",
    "\n",
    "list = struct_llm.invoke(\"Give me alist of 5 things I can do with Smithery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c3115d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v__positional_only': [\"1. **Custom Craftsmanship:**\\n   - Commission Smithery to create unique, handcrafted metal items tailored to your specifications. Be it architectural elements, bespoke furniture, decorative art pieces, or more, Smithery's craftsmanship can bring intricate designs to life.\",\n",
       "  \"2. **Workshops & Classes:**\\n   - Participate in hands-on workshops that allow you to learn the basics of smithing and metalwork. Whether you're a hobbyist or aspiring artisan, these sessions offer insight into techniques and skills used in the trade.\",\n",
       "  \"3. **Restoration of Metalworks:**\\n   - Utilize Smithery's services to restore and refurbish antique or damaged metal items. From intricate heirloom jewelry to larger forged gateworks, they can revive and preserve the original beauty of treasured pieces.\",\n",
       "  '4. **Collaborative Projects:**\\n   - Engage Smithery in collaborative design endeavors with architects, designers, or other artists. Utilize their expertise to incorporate wrought iron, hand-forged details, and artistic metal elements into broader creative projects.',\n",
       "  \"5. **Product Retail:**\\n   - Explore and purchase from Smithery's collection of ready-made metal goods. Whether looking for decorative items, functional pieces, or gifts, their curated collection offers craftsmanship combined with quality materials.\"]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5215c771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install smithery, langchain_mcp_tools\n",
    "import smithery\n",
    "import mcp\n",
    "from langchain_mcp_tools import convert_mcp_to_langchain_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# MCP To LangChain Tools Conversion Utility, install langchain_mcp_tools\n",
    "# https://pypi.org/project/langchain-mcp-tools/ \n",
    "\n",
    "# login to Smithery: https://smithery.ai/\n",
    "# create a API Key\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7740277f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m MCP server \"filesystem\": initializing with: {'command': 'npx', 'args': ['-y', '@modelcontextprotocol/server-filesystem', '.']}\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"sequential-thinking\": initializing with: {'command': 'npx', 'args': ['-y', '@modelcontextprotocol/server-sequential-thinking']}\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"filesystem\": connected\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"filesystem\": 11 tool(s) available:\n",
      "\u001b[90m[INFO]\u001b[0m - read_file\n",
      "\u001b[90m[INFO]\u001b[0m - read_multiple_files\n",
      "\u001b[90m[INFO]\u001b[0m - write_file\n",
      "\u001b[90m[INFO]\u001b[0m - edit_file\n",
      "\u001b[90m[INFO]\u001b[0m - create_directory\n",
      "\u001b[90m[INFO]\u001b[0m - list_directory\n",
      "\u001b[90m[INFO]\u001b[0m - directory_tree\n",
      "\u001b[90m[INFO]\u001b[0m - move_file\n",
      "\u001b[90m[INFO]\u001b[0m - search_files\n",
      "\u001b[90m[INFO]\u001b[0m - get_file_info\n",
      "\u001b[90m[INFO]\u001b[0m - list_allowed_directories\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"sequential-thinking\": connected\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"sequential-thinking\": 1 tool(s) available:\n",
      "\u001b[90m[INFO]\u001b[0m - sequentialthinking\n",
      "\u001b[90m[INFO]\u001b[0m MCP servers initialized: 12 tool(s) available in total\n"
     ]
    }
   ],
   "source": [
    "mcp_servers = {\n",
    "    \"filesystem\": {\n",
    "        \"command\": \"npx\",\n",
    "        \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \".\"]\n",
    "    },\n",
    "    \"sequential-thinking\": {\n",
    "        \"command\": \"npx\",\n",
    "        \"args\": [\n",
    "            \"-y\",\n",
    "            \"@modelcontextprotocol/server-sequential-thinking\"\n",
    "        ]\n",
    "    }, \n",
    "    }\n",
    "\n",
    "tools, cleanup = await convert_mcp_to_langchain_tools(\n",
    "    mcp_servers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b8f68f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- read_file: Read the complete contents of a file from the file system. Handles various text encodings and provides detailed error messages if the file cannot be read. Use this tool when you need to examine the contents of a single file. Only works within allowed directories.',\n",
       " \"- read_multiple_files: Read the contents of multiple files simultaneously. This is more efficient than reading files one by one when you need to analyze or compare multiple files. Each file's content is returned with its path as a reference. Failed reads for individual files won't stop the entire operation. Only works within allowed directories.\",\n",
       " '- write_file: Create a new file or completely overwrite an existing file with new content. Use with caution as it will overwrite existing files without warning. Handles text content with proper encoding. Only works within allowed directories.',\n",
       " '- edit_file: Make line-based edits to a text file. Each edit replaces exact line sequences with new content. Returns a git-style diff showing the changes made. Only works within allowed directories.',\n",
       " '- create_directory: Create a new directory or ensure a directory exists. Can create multiple nested directories in one operation. If the directory already exists, this operation will succeed silently. Perfect for setting up directory structures for projects or ensuring required paths exist. Only works within allowed directories.',\n",
       " '- list_directory: Get a detailed listing of all files and directories in a specified path. Results clearly distinguish between files and directories with [FILE] and [DIR] prefixes. This tool is essential for understanding directory structure and finding specific files within a directory. Only works within allowed directories.',\n",
       " \"- directory_tree: Get a recursive tree view of files and directories as a JSON structure. Each entry includes 'name', 'type' (file/directory), and 'children' for directories. Files have no children array, while directories always have a children array (which may be empty). The output is formatted with 2-space indentation for readability. Only works within allowed directories.\",\n",
       " '- move_file: Move or rename files and directories. Can move files between directories and rename them in a single operation. If the destination exists, the operation will fail. Works across different directories and can be used for simple renaming within the same directory. Both source and destination must be within allowed directories.',\n",
       " \"- search_files: Recursively search for files and directories matching a pattern. Searches through all subdirectories from the starting path. The search is case-insensitive and matches partial names. Returns full paths to all matching items. Great for finding files when you don't know their exact location. Only searches within allowed directories.\",\n",
       " '- get_file_info: Retrieve detailed metadata about a file or directory. Returns comprehensive information including size, creation time, last modified time, permissions, and type. This tool is perfect for understanding file characteristics without reading the actual content. Only works within allowed directories.',\n",
       " '- list_allowed_directories: Returns the list of directories that this server is allowed to access. Use this to understand which directories are available before trying to access files.',\n",
       " '- sequentialthinking: A detailed tool for dynamic and reflective problem-solving through thoughts.',\n",
       " 'This tool helps analyze problems through a flexible thinking process that can adapt and evolve.',\n",
       " 'Each thought can build on, question, or revise previous insights as understanding deepens.',\n",
       " '',\n",
       " 'When to use this tool:',\n",
       " '- Breaking down complex problems into steps',\n",
       " '- Planning and design with room for revision',\n",
       " '- Analysis that might need course correction',\n",
       " '- Problems where the full scope might not be clear initially',\n",
       " '- Problems that require a multi-step solution',\n",
       " '- Tasks that need to maintain context over multiple steps',\n",
       " '- Situations where irrelevant information needs to be filtered out',\n",
       " '',\n",
       " 'Key features:',\n",
       " '- You can adjust total_thoughts up or down as you progress',\n",
       " '- You can question or revise previous thoughts',\n",
       " '- You can add more thoughts even after reaching what seemed like the end',\n",
       " '- You can express uncertainty and explore alternative approaches',\n",
       " '- Not every thought needs to build linearly - you can branch or backtrack',\n",
       " '- Generates a solution hypothesis',\n",
       " '- Verifies the hypothesis based on the Chain of Thought steps',\n",
       " '- Repeats the process until satisfied',\n",
       " '- Provides a correct answer',\n",
       " '',\n",
       " 'Parameters explained:',\n",
       " '- thought: Your current thinking step, which can include:',\n",
       " '* Regular analytical steps',\n",
       " '* Revisions of previous thoughts',\n",
       " '* Questions about previous decisions',\n",
       " '* Realizations about needing more analysis',\n",
       " '* Changes in approach',\n",
       " '* Hypothesis generation',\n",
       " '* Hypothesis verification',\n",
       " '- next_thought_needed: True if you need more thinking, even if at what seemed like the end',\n",
       " '- thought_number: Current number in sequence (can go beyond initial total if needed)',\n",
       " '- total_thoughts: Current estimate of thoughts needed (can be adjusted up/down)',\n",
       " '- is_revision: A boolean indicating if this thought revises previous thinking',\n",
       " '- revises_thought: If is_revision is true, which thought number is being reconsidered',\n",
       " '- branch_from_thought: If branching, which thought number is the branching point',\n",
       " '- branch_id: Identifier for the current branch (if any)',\n",
       " '- needs_more_thoughts: If reaching end but realizing more thoughts needed',\n",
       " '',\n",
       " 'You should:',\n",
       " '1. Start with an initial estimate of needed thoughts, but be ready to adjust',\n",
       " '2. Feel free to question or revise previous thoughts',\n",
       " '3. Don\\'t hesitate to add more thoughts if needed, even at the \"end\"',\n",
       " '4. Express uncertainty when present',\n",
       " '5. Mark thoughts that revise previous thinking or branch into new paths',\n",
       " '6. Ignore information that is irrelevant to the current step',\n",
       " '7. Generate a solution hypothesis when appropriate',\n",
       " '8. Verify the hypothesis based on the Chain of Thought steps',\n",
       " '9. Repeat the process until satisfied with the solution',\n",
       " '10. Provide a single, ideally correct answer as the final output',\n",
       " '11. Only set next_thought_needed to false when truly done and a satisfactory answer is reached']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools_by_name_str = \"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools]).split(\"\\n\")\n",
    "tools_by_name_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d33a4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m MCP server \"filesystem\": initializing with: {'command': 'npx', 'args': ['@modelcontextprotocol/server-filesystem', '/home/teodorrk/projects/Bolt-x-OpenBridge/fastagent/teodor/langchain_mcp/notebooks']}\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"filesystem\": connected\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"filesystem\": 11 tool(s) available:\n",
      "\u001b[90m[INFO]\u001b[0m - read_file\n",
      "\u001b[90m[INFO]\u001b[0m - read_multiple_files\n",
      "\u001b[90m[INFO]\u001b[0m - write_file\n",
      "\u001b[90m[INFO]\u001b[0m - edit_file\n",
      "\u001b[90m[INFO]\u001b[0m - create_directory\n",
      "\u001b[90m[INFO]\u001b[0m - list_directory\n",
      "\u001b[90m[INFO]\u001b[0m - directory_tree\n",
      "\u001b[90m[INFO]\u001b[0m - move_file\n",
      "\u001b[90m[INFO]\u001b[0m - search_files\n",
      "\u001b[90m[INFO]\u001b[0m - get_file_info\n",
      "\u001b[90m[INFO]\u001b[0m - list_allowed_directories\n",
      "\u001b[90m[INFO]\u001b[0m MCP servers initialized: 11 tool(s) available in total\n",
      "\u001b[90m[INFO]\u001b[0m HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[90m[INFO]\u001b[0m MCP tool \"filesystem\"/\"list_allowed_directories\" received input: {'path': '.'}\n",
      "\u001b[90m[INFO]\u001b[0m MCP tool \"filesystem\"/\"list_allowed_directories\" received result (size: 17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The\n",
      "The current\n",
      "The current directory\n",
      "The current directory contains\n",
      "The current directory contains the\n",
      "The current directory contains the following\n",
      "The current directory contains the following file\n",
      "The current directory contains the following file:\n",
      "\n",
      "\n",
      "The current directory contains the following file:\n",
      "\n",
      "-\n",
      "The current directory contains the following file:\n",
      "\n",
      "- **\n",
      "The current directory contains the following file:\n",
      "\n",
      "- **test\n",
      "The current directory contains the following file:\n",
      "\n",
      "- **test.ip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m MCP server \"filesystem\": session closed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current directory contains the following file:\n",
      "\n",
      "- **test.ipyn\n",
      "The current directory contains the following file:\n",
      "\n",
      "- **test.ipynb\n",
      "The current directory contains the following file:\n",
      "\n",
      "- **test.ipynb**\n",
      "The current directory contains the following file:\n",
      "\n",
      "- **test.ipynb**\n",
      "MCP server sessions closed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_mcp_tools import convert_mcp_to_langchain_tools\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "\n",
    "class MCPAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.mcp_servers = {\n",
    "            \"filesystem\": {\n",
    "                \"command\": \"npx\",\n",
    "                \"args\": [\"@modelcontextprotocol/server-filesystem\", os.getcwd()]\n",
    "            }\n",
    "        }\n",
    "        self.tools = None\n",
    "        self.agent = None\n",
    "        self.cleanup_func = None\n",
    "        self.config = {\"configurable\": {\"thread_id\": \"42\"}}\n",
    "\n",
    "    async def initialize(self):\n",
    "        self.tools, self.cleanup_func = await convert_mcp_to_langchain_tools(self.mcp_servers)\n",
    "        if self.tools:\n",
    "            self.agent = create_react_agent(self.llm, self.tools)\n",
    "        else:\n",
    "            print(\"Warning: No tools were loaded from MCP servers.\")\n",
    "\n",
    "    async def cleanup(self):\n",
    "        if self.cleanup_func:\n",
    "            await self.cleanup_func()\n",
    "            print(\"MCP server sessions closed.\")\n",
    "\n",
    "    async def astream_events(self, query, config, chat_history=None, return_direct=False):\n",
    "        if not self.agent:\n",
    "            print(\"Agent not initialized.\")\n",
    "            return\n",
    "        inputs = {\"messages\": [HumanMessage(content=query)]}\n",
    "        async for chunk in self.agent.astream_events(inputs, config, stream_mode=\"values\"):\n",
    "            yield chunk\n",
    "\n",
    "#async def main():\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# Create the MCPAgent instance\n",
    "mcp_agent = MCPAgent(llm)\n",
    "\n",
    "# Initialize the agent and load tools\n",
    "await mcp_agent.initialize()\n",
    "\n",
    "message = \"List all files in the current directory\"\n",
    "\n",
    "message \n",
    "if mcp_agent.agent:\n",
    "    event_stream = mcp_agent.astream_events(\n",
    "        query=message,\n",
    "        config=mcp_agent.config,\n",
    "    )\n",
    "    content = \"\"\n",
    "    async for event in event_stream:\n",
    "        #if isinstance(chunk, dict):\n",
    "        kind = event[\"event\"]\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            chunk = event[\"data\"][\"chunk\"]\n",
    "            content += chunk.content\n",
    "            #print(repr(content))\n",
    "            print(content)\n",
    "            #message = chunk[\"messages\"][-1]\n",
    "            #if hasattr(message, 'content'):\n",
    "            #    print(message.content)\n",
    "\n",
    "# Cleanup the MCP server sessions\n",
    "await mcp_agent.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "858f0ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m MCP server \"server1\": initializing with: {'url': 'wss://server.smithery.ai/@swaroopkasaraneni/mcp-servers/ws?config=e30%3D&api_key=8c280430-cd8c-4708-ae5d-b87d0ba3345'}\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"server2\": initializing with: {'url': 'wss://server.smithery.ai/@mcp-examples/weather/ws?config=e30%3D&api_key=8c280430-cd8c-4708-ae5d-b87d0ba3345'}\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"server1\": connected\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"server1\": 2 tool(s) available:\n",
      "\u001b[90m[INFO]\u001b[0m - add\n",
      "\u001b[90m[INFO]\u001b[0m - multiply\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"server2\": connected\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"server2\": 2 tool(s) available:\n",
      "\u001b[90m[INFO]\u001b[0m - get-alerts\n",
      "\u001b[90m[INFO]\u001b[0m - get-forecast\n",
      "\u001b[90m[INFO]\u001b[0m MCP servers initialized: 4 tool(s) available in total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools available:\n",
      "add multiply get-alerts get-forecast\n"
     ]
    }
   ],
   "source": [
    "# Do not forget to provide your api key here\n",
    "url1 = smithery.create_smithery_url(\"wss://server.smithery.ai/@swaroopkasaraneni/mcp-servers/ws\", {}) + SMITHERY_API_KEY\n",
    "\n",
    "url2 = smithery.create_smithery_url(\"wss://server.smithery.ai/@mcp-examples/weather/ws\", {}) + SMITHERY_API_KEY\n",
    "\n",
    "mcp_servers = {\n",
    "    \"server1\": {\"url\": url1},\n",
    "    \"server2\": {\"url\": url2},\n",
    "}\n",
    "\n",
    "tools, cleanup = await convert_mcp_to_langchain_tools(\n",
    "    mcp_servers\n",
    ")\n",
    "\n",
    "print(\"Tools available:\")\n",
    "print(tools[0].get_name(), tools[1].get_name(), tools[2].get_name(), tools[3].get_name())\n",
    "\n",
    "# Tools available:\n",
    "# add multiply get-alerts get-forecast\n",
    "\n",
    "# creating an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2569d136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m MCP server \"file_server\": initializing with: {'url': 'wss://server.smithery.ai/@bunasQ/fs/ws?config=e30%3D&api_key=8c280430-cd8c-4708-ae5d-b87d0ba3345'}\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"context_server\": initializing with: {'url': 'wss://server.smithery.ai/@bsmi021/mcp-file-context-server/ws?config=e30%3D&api_key=8c280430-cd8c-4708-ae5d-b87d0ba3345'}\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"file_server\": connected\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"file_server\": 1 tool(s) available:\n",
      "\u001b[90m[INFO]\u001b[0m - read-file-21\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"context_server\": connected\n",
      "\u001b[90m[INFO]\u001b[0m MCP server \"context_server\": 5 tool(s) available:\n",
      "\u001b[90m[INFO]\u001b[0m - read_context\n",
      "\u001b[90m[INFO]\u001b[0m - get_chunk_count\n",
      "\u001b[90m[INFO]\u001b[0m - set_profile\n",
      "\u001b[90m[INFO]\u001b[0m - get_profile_context\n",
      "\u001b[90m[INFO]\u001b[0m - generate_outline\n",
      "\u001b[90m[INFO]\u001b[0m MCP servers initialized: 6 tool(s) available in total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tools: read-file-21, read_context, get_chunk_count, set_profile, get_profile_context, generate_outline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[90m[INFO]\u001b[0m MCP tool \"context_server\"/\"generate_outline\" received input: {'path': '/home/teodorrk/projects/openbridge-mvp/teodor/mcp-agent-langgraph/requirements.txt'}\n",
      "\u001b[90m[WARNING]\u001b[0m MCP tool \"context_server\"/\"generate_outline\" caused error:  Tool execution failed: [TextContent(type='text', text=\"File operation error: Failed to read file: ENOENT: no such file or directory, stat '/home/teodorrk/projects/openbridge-mvp/teodor/mcp-agent-langgraph/requirements.txt' (UNKNOWN_ERROR)\", annotations=None)]\n",
      "\u001b[90m[INFO]\u001b[0m HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[90m[INFO]\u001b[0m Retrying request to /chat/completions in 0.243000 seconds\n",
      "\u001b[90m[INFO]\u001b[0m HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "\u001b[90m[INFO]\u001b[0m Retrying request to /chat/completions in 0.243000 seconds\n",
      "\u001b[90m[INFO]\u001b[0m HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-535DL63px2GlJwXoOpldBPle on tokens per min (TPM): Limit 30000, Used 30000, Requested 122. Please try again in 243ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     21\u001b[39m file_agent = create_react_agent(\n\u001b[32m     22\u001b[39m     llm,\n\u001b[32m     23\u001b[39m     tools\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m message = \u001b[33m\"\u001b[39m\u001b[33mCan you read the contents of the file \u001b[39m\u001b[33m'\u001b[39m\u001b[33mrequirements.txt\u001b[39m\u001b[33m'\u001b[39m\u001b[33m in this path: \u001b[39m\u001b[33m'\u001b[39m\u001b[33m/home/teodorrk/projects/openbridge-mvp/teodor/mcp-agent-langgraph/requirements.txt\u001b[39m\u001b[33m'\u001b[39m\u001b[33m?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m agent_response = \u001b[38;5;28;01mawait\u001b[39;00m file_agent.ainvoke({\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: message})\n\u001b[32m     27\u001b[39m agent_response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2773\u001b[39m, in \u001b[36mPregel.ainvoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2771\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2772\u001b[39m     chunks = []\n\u001b[32m-> \u001b[39m\u001b[32m2773\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.astream(\n\u001b[32m   2774\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2775\u001b[39m     config,\n\u001b[32m   2776\u001b[39m     stream_mode=stream_mode,\n\u001b[32m   2777\u001b[39m     output_keys=output_keys,\n\u001b[32m   2778\u001b[39m     interrupt_before=interrupt_before,\n\u001b[32m   2779\u001b[39m     interrupt_after=interrupt_after,\n\u001b[32m   2780\u001b[39m     checkpoint_during=checkpoint_during,\n\u001b[32m   2781\u001b[39m     debug=debug,\n\u001b[32m   2782\u001b[39m     **kwargs,\n\u001b[32m   2783\u001b[39m ):\n\u001b[32m   2784\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode == \u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2785\u001b[39m         latest = chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2655\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2649\u001b[39m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[32m   2650\u001b[39m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[32m   2651\u001b[39m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[32m   2652\u001b[39m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[32m   2653\u001b[39m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[32m   2654\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m loop.tick(input_keys=\u001b[38;5;28mself\u001b[39m.input_channels):\n\u001b[32m-> \u001b[39m\u001b[32m2655\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2656\u001b[39m         loop.tasks.values(),\n\u001b[32m   2657\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2658\u001b[39m         retry_policy=\u001b[38;5;28mself\u001b[39m.retry_policy,\n\u001b[32m   2659\u001b[39m         get_waiter=get_waiter,\n\u001b[32m   2660\u001b[39m     ):\n\u001b[32m   2661\u001b[39m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2662\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[32m   2663\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langgraph/prebuilt/chat_agent_executor.py:763\u001b[39m, in \u001b[36mcreate_react_agent.<locals>.acall_model\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34macall_model\u001b[39m(state: StateSchema, config: RunnableConfig) -> StateSchema:\n\u001b[32m    762\u001b[39m     state = _get_model_input_state(state)\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m     response = cast(AIMessage, \u001b[38;5;28;01mawait\u001b[39;00m model_runnable.ainvoke(state, config))\n\u001b[32m    764\u001b[39m     \u001b[38;5;66;03m# add agent name to the AIMessage\u001b[39;00m\n\u001b[32m    765\u001b[39m     response.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3089\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3087\u001b[39m     part = functools.partial(step.ainvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[32m   3088\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m asyncio_accepts_context():\n\u001b[32m-> \u001b[39m\u001b[32m3089\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(part(), context=context)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   3090\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3091\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(part())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_core/runnables/base.py:5453\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5446\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5447\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5448\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5451\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5452\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5453\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5454\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5455\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5456\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5457\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:353\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    345\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m     **kwargs: Any,\n\u001b[32m    351\u001b[39m ) -> BaseMessage:\n\u001b[32m    352\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    354\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    355\u001b[39m         stop=stop,\n\u001b[32m    356\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    357\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    358\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    359\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    360\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    361\u001b[39m         **kwargs,\n\u001b[32m    362\u001b[39m     )\n\u001b[32m    363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:905\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    896\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    898\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    902\u001b[39m     **kwargs: Any,\n\u001b[32m    903\u001b[39m ) -> LLMResult:\n\u001b[32m    904\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    906\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    907\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:863\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    850\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m    851\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    852\u001b[39m             *[\n\u001b[32m    853\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m    861\u001b[39m             ]\n\u001b[32m    862\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m    864\u001b[39m flattened_outputs = [\n\u001b[32m    865\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[list-item, union-attr]\u001b[39;00m\n\u001b[32m    866\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m    867\u001b[39m ]\n\u001b[32m    868\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1033\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1031\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1032\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1033\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1034\u001b[39m             messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1035\u001b[39m         )\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1037\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:1157\u001b[39m, in \u001b[36mBaseChatOpenAI._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1155\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m   1156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1157\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client.create(**payload)\n\u001b[32m   1158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(\n\u001b[32m   1159\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m._create_chat_result, response, generation_info\n\u001b[32m   1160\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:2000\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1957\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1958\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1959\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1997\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1998\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   1999\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2000\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2001\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2002\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2003\u001b[39m             {\n\u001b[32m   2004\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2005\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2006\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2007\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2008\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2009\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2010\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2011\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2012\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2013\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2014\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2015\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2016\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2017\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2018\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2019\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2020\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2021\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2022\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2023\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2024\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2025\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2026\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2027\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2028\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2029\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2030\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2031\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2032\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2033\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2034\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2035\u001b[39m             },\n\u001b[32m   2036\u001b[39m             completion_create_params.CompletionCreateParams,\n\u001b[32m   2037\u001b[39m         ),\n\u001b[32m   2038\u001b[39m         options=make_request_options(\n\u001b[32m   2039\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2040\u001b[39m         ),\n\u001b[32m   2041\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2042\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2043\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2044\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/openai/_base_client.py:1767\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1753\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1754\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1755\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1762\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1763\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1764\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1765\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1766\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/openai/_base_client.py:1461\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[39m\n\u001b[32m   1458\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1459\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1462\u001b[39m     cast_to=cast_to,\n\u001b[32m   1463\u001b[39m     options=options,\n\u001b[32m   1464\u001b[39m     stream=stream,\n\u001b[32m   1465\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1466\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1467\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/openai/_base_client.py:1547\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m err.response.aclose()\n\u001b[32m-> \u001b[39m\u001b[32m1547\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry_request(\n\u001b[32m   1548\u001b[39m         input_options,\n\u001b[32m   1549\u001b[39m         cast_to,\n\u001b[32m   1550\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1551\u001b[39m         response_headers=err.response.headers,\n\u001b[32m   1552\u001b[39m         stream=stream,\n\u001b[32m   1553\u001b[39m         stream_cls=stream_cls,\n\u001b[32m   1554\u001b[39m     )\n\u001b[32m   1556\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err.response.is_closed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1590\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m   1592\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anyio.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1595\u001b[39m     options=options,\n\u001b[32m   1596\u001b[39m     cast_to=cast_to,\n\u001b[32m   1597\u001b[39m     retries_taken=retries_taken + \u001b[32m1\u001b[39m,\n\u001b[32m   1598\u001b[39m     stream=stream,\n\u001b[32m   1599\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1600\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/openai/_base_client.py:1547\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m err.response.aclose()\n\u001b[32m-> \u001b[39m\u001b[32m1547\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry_request(\n\u001b[32m   1548\u001b[39m         input_options,\n\u001b[32m   1549\u001b[39m         cast_to,\n\u001b[32m   1550\u001b[39m         retries_taken=retries_taken,\n\u001b[32m   1551\u001b[39m         response_headers=err.response.headers,\n\u001b[32m   1552\u001b[39m         stream=stream,\n\u001b[32m   1553\u001b[39m         stream_cls=stream_cls,\n\u001b[32m   1554\u001b[39m     )\n\u001b[32m   1556\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err.response.is_closed:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient._retry_request\u001b[39m\u001b[34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[39m\n\u001b[32m   1590\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m   1592\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anyio.sleep(timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\n\u001b[32m   1595\u001b[39m     options=options,\n\u001b[32m   1596\u001b[39m     cast_to=cast_to,\n\u001b[32m   1597\u001b[39m     retries_taken=retries_taken + \u001b[32m1\u001b[39m,\n\u001b[32m   1598\u001b[39m     stream=stream,\n\u001b[32m   1599\u001b[39m     stream_cls=stream_cls,\n\u001b[32m   1600\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/openai/_base_client.py:1562\u001b[39m, in \u001b[36mAsyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[39m\n\u001b[32m   1559\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1561\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1565\u001b[39m     cast_to=cast_to,\n\u001b[32m   1566\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1570\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1571\u001b[39m )\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-535DL63px2GlJwXoOpldBPle on tokens per min (TPM): Limit 30000, Used 30000, Requested 122. Please try again in 243ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
      "During task with name 'agent' and id '79995ec5-cefe-9f86-9d2a-2c8d0b79fb10'"
     ]
    }
   ],
   "source": [
    "import smithery\n",
    "import mcp\n",
    "from mcp.client.websocket import websocket_client\n",
    "\n",
    "# Create Smithery URL with server endpoint\n",
    "file_url = smithery.create_smithery_url(\"wss://server.smithery.ai/@bunasQ/fs/ws\", {}) + SMITHERY_API_KEY\n",
    "\n",
    "context_url = url = smithery.create_smithery_url(\"wss://server.smithery.ai/@bsmi021/mcp-file-context-server/ws\", {}) + SMITHERY_API_KEY\n",
    "\n",
    "\n",
    "mcp_servers = {\n",
    "    \"file_server\": {\"url\": file_url},\n",
    "    \"context_server\": {\"url\": context_url},\n",
    "}\n",
    "\n",
    "tools, cleanup = await convert_mcp_to_langchain_tools(\n",
    "    mcp_servers\n",
    ")\n",
    "print(f\"Available tools: {', '.join([t.name for t in tools])}\")\n",
    "\n",
    "file_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools\n",
    ")\n",
    "message = \"Can you read the contents of the file 'requirements.txt' in this path: '/home/teodorrk/projects/openbridge-mvp/teodor/mcp-agent-langgraph/requirements.txt'?\"\n",
    "agent_response = await file_agent.ainvoke({\"messages\": message})\n",
    "agent_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1d85e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = smithery.create_smithery_url(\"wss://server.smithery.ai/@bsmi021/mcp-file-context-server/ws\", {}) + \"&api_key=your-smithery-api-key\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abad05f",
   "metadata": {},
   "source": [
    "## Github Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee49596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m MCP server \"git_server\": initializing with: {'url': 'wss://server.smithery.ai/@smithery-ai/github/ws?config=e30%3D8c280430-cd8c-4708-ae5d-b87d0ba3345'}\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWouldBlock\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/anyio/streams/memory.py:111\u001b[39m, in \u001b[36mMemoryObjectReceiveStream.receive\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreceive_nowait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m WouldBlock:\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# Add ourselves in the queue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/anyio/streams/memory.py:106\u001b[39m, in \u001b[36mMemoryObjectReceiveStream.receive_nowait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m WouldBlock\n",
      "\u001b[31mWouldBlock\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m git_url = smithery.create_smithery_url(\u001b[33m\"\u001b[39m\u001b[33mwss://server.smithery.ai/@smithery-ai/github/ws\u001b[39m\u001b[33m\"\u001b[39m, {\n\u001b[32m      3\u001b[39m }) + SMITHERY_API_KEY\n\u001b[32m      5\u001b[39m mcp_servers = {\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgit_server\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m: git_url},\n\u001b[32m      7\u001b[39m }\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m tools, cleanup = \u001b[38;5;28;01mawait\u001b[39;00m convert_mcp_to_langchain_tools(\n\u001b[32m     10\u001b[39m     mcp_servers\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvailable tools: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([t.name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mt\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mtools])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py:389\u001b[39m, in \u001b[36mconvert_mcp_to_langchain_tools\u001b[39m\u001b[34m(server_configs, logger)\u001b[39m\n\u001b[32m    383\u001b[39m langchain_tools: \u001b[38;5;28mlist\u001b[39m[BaseTool] = []\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (server_name, server_config), transport \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[32m    385\u001b[39m     server_configs.items(),\n\u001b[32m    386\u001b[39m     transports,\n\u001b[32m    387\u001b[39m     strict=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    388\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m389\u001b[39m     tools = \u001b[38;5;28;01mawait\u001b[39;00m get_mcp_server_tools(\n\u001b[32m    390\u001b[39m         server_name,\n\u001b[32m    391\u001b[39m         transport,\n\u001b[32m    392\u001b[39m         async_exit_stack,\n\u001b[32m    393\u001b[39m         logger\n\u001b[32m    394\u001b[39m     )\n\u001b[32m    395\u001b[39m     langchain_tools.extend(tools)\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# Define a cleanup function to properly shut down all servers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py:204\u001b[39m, in \u001b[36mget_mcp_server_tools\u001b[39m\u001b[34m(server_name, transport, exit_stack, logger)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# Initialize client session with cleanup logging\u001b[39;00m\n\u001b[32m    197\u001b[39m session = \u001b[38;5;28;01mawait\u001b[39;00m exit_stack.enter_async_context(\n\u001b[32m    198\u001b[39m     log_before_aexit(\n\u001b[32m    199\u001b[39m         ClientSession(read, write),\n\u001b[32m    200\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMCP server \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m: session closed\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    201\u001b[39m     )\n\u001b[32m    202\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m session.initialize()\n\u001b[32m    205\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMCP server \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m: connected\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Get MCP tools\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/mcp/client/session.py:122\u001b[39m, in \u001b[36mClientSession.initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    114\u001b[39m sampling = types.SamplingCapability()\n\u001b[32m    115\u001b[39m roots = types.RootsCapability(\n\u001b[32m    116\u001b[39m     \u001b[38;5;66;03m# TODO: Should this be based on whether we\u001b[39;00m\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# _will_ send notifications, or only whether\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# they're supported?\u001b[39;00m\n\u001b[32m    119\u001b[39m     listChanged=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    120\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send_request(\n\u001b[32m    123\u001b[39m     types.ClientRequest(\n\u001b[32m    124\u001b[39m         types.InitializeRequest(\n\u001b[32m    125\u001b[39m             method=\u001b[33m\"\u001b[39m\u001b[33minitialize\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    126\u001b[39m             params=types.InitializeRequestParams(\n\u001b[32m    127\u001b[39m                 protocolVersion=types.LATEST_PROTOCOL_VERSION,\n\u001b[32m    128\u001b[39m                 capabilities=types.ClientCapabilities(\n\u001b[32m    129\u001b[39m                     sampling=sampling,\n\u001b[32m    130\u001b[39m                     experimental=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    131\u001b[39m                     roots=roots,\n\u001b[32m    132\u001b[39m                 ),\n\u001b[32m    133\u001b[39m                 clientInfo=types.Implementation(name=\u001b[33m\"\u001b[39m\u001b[33mmcp\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m0.1.0\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    134\u001b[39m             ),\n\u001b[32m    135\u001b[39m         )\n\u001b[32m    136\u001b[39m     ),\n\u001b[32m    137\u001b[39m     types.InitializeResult,\n\u001b[32m    138\u001b[39m )\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.protocolVersion \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SUPPORTED_PROTOCOL_VERSIONS:\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    142\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnsupported protocol version from the server: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    143\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.protocolVersion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    144\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/mcp/shared/session.py:252\u001b[39m, in \u001b[36mBaseSession.send_request\u001b[39m\u001b[34m(self, request, result_type)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(\n\u001b[32m    248\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    249\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read_timeout_seconds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    250\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read_timeout_seconds.total_seconds()\n\u001b[32m    251\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m         response_or_error = \u001b[38;5;28;01mawait\u001b[39;00m response_stream_reader.receive()\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m McpError(\n\u001b[32m    255\u001b[39m         ErrorData(\n\u001b[32m    256\u001b[39m             code=httpx.codes.REQUEST_TIMEOUT,\n\u001b[32m   (...)\u001b[39m\u001b[32m    262\u001b[39m         )\n\u001b[32m    263\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/anyio/streams/memory.py:119\u001b[39m, in \u001b[36mMemoryObjectReceiveStream.receive\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._state.waiting_receivers[receive_event] = receiver\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m receive_event.wait()\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m._state.waiting_receivers.pop(receive_event, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/locks.py:213\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Create Smithery URL with server endpoint\n",
    "git_url = smithery.create_smithery_url(\"wss://server.smithery.ai/@smithery-ai/github/ws\", {\n",
    "  \"githubPersonalAccessToken\": GITHUB_TOKEN\n",
    "}) + SMITHERY_API_KEY\n",
    "\n",
    "mcp_servers = {\n",
    "    \"git_server\": {\"url\": git_url},\n",
    "}\n",
    "\n",
    "tools, cleanup = await convert_mcp_to_langchain_tools(\n",
    "    mcp_servers\n",
    ")\n",
    "\n",
    "print(f\"Available tools: {', '.join([t.name for t in tools])}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb5ea63c",
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mWouldBlock\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/anyio/streams/memory.py:111\u001b[39m, in \u001b[36mMemoryObjectReceiveStream.receive\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreceive_nowait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m WouldBlock:\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# Add ourselves in the queue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/anyio/streams/memory.py:106\u001b[39m, in \u001b[36mMemoryObjectReceiveStream.receive_nowait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m EndOfStream\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m WouldBlock\n",
      "\u001b[31mWouldBlock\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     16\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvailable tools: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([t.name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mt\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mtools_result])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m             \u001b[38;5;66;03m# # Example: Fetch repository info (adjust tool_name and params)\u001b[39;00m\n\u001b[32m     19\u001b[39m             \u001b[38;5;66;03m# repo_info = await session.call_tool(\u001b[39;00m\n\u001b[32m     20\u001b[39m             \u001b[38;5;66;03m#     \"get_repository\",  # Hypothetical tool name (check actual available tools)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m             \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     38\u001b[39m             \u001b[38;5;66;03m# print(\"Created issue:\", new_issue)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m github_operations() \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mgithub_operations\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgithub_operations\u001b[39m():\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Connect to the server using websocket client\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m websocket_client(url) \u001b[38;5;28;01mas\u001b[39;00m streams:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m mcp.ClientSession(*streams) \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m     14\u001b[39m             \u001b[38;5;66;03m# List available tools (optional, to see what's available)\u001b[39;00m\n\u001b[32m     15\u001b[39m             tools_result = \u001b[38;5;28;01mawait\u001b[39;00m session.list_tools()\n\u001b[32m     16\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvailable tools: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([t.name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mt\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mtools_result])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/mcp/shared/session.py:210\u001b[39m, in \u001b[36mBaseSession.__aexit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# Using BaseSession as a context manager should not block on exit (this\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# would be very surprising behavior), so make sure to cancel the tasks\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# in the task group.\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[38;5;28mself\u001b[39m._task_group.cancel_scope.cancel()\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._task_group.\u001b[34m__aexit__\u001b[39m(exc_type, exc_val, exc_tb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py:776\u001b[39m, in \u001b[36mTaskGroup.__aexit__\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    772\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m BaseExceptionGroup(\n\u001b[32m    773\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33munhandled errors in a TaskGroup\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._exceptions\n\u001b[32m    774\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    775\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m exc_val:\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[32m    777\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    778\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cancel_scope.\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mtype\u001b[39m(exc), exc, exc.__traceback__):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mgithub_operations\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m websocket_client(url) \u001b[38;5;28;01mas\u001b[39;00m streams:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m mcp.ClientSession(*streams) \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m     14\u001b[39m         \u001b[38;5;66;03m# List available tools (optional, to see what's available)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m         tools_result = \u001b[38;5;28;01mawait\u001b[39;00m session.list_tools()\n\u001b[32m     16\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAvailable tools: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join([t.name\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mt\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mtools_result])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/mcp/client/session.py:312\u001b[39m, in \u001b[36mClientSession.list_tools\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist_tools\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> types.ListToolsResult:\n\u001b[32m    311\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a tools/list request.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m312\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send_request(\n\u001b[32m    313\u001b[39m         types.ClientRequest(\n\u001b[32m    314\u001b[39m             types.ListToolsRequest(\n\u001b[32m    315\u001b[39m                 method=\u001b[33m\"\u001b[39m\u001b[33mtools/list\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    316\u001b[39m             )\n\u001b[32m    317\u001b[39m         ),\n\u001b[32m    318\u001b[39m         types.ListToolsResult,\n\u001b[32m    319\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/mcp/shared/session.py:252\u001b[39m, in \u001b[36mBaseSession.send_request\u001b[39m\u001b[34m(self, request, result_type)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(\n\u001b[32m    248\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    249\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read_timeout_seconds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    250\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read_timeout_seconds.total_seconds()\n\u001b[32m    251\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m         response_or_error = \u001b[38;5;28;01mawait\u001b[39;00m response_stream_reader.receive()\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m McpError(\n\u001b[32m    255\u001b[39m         ErrorData(\n\u001b[32m    256\u001b[39m             code=httpx.codes.REQUEST_TIMEOUT,\n\u001b[32m   (...)\u001b[39m\u001b[32m    262\u001b[39m         )\n\u001b[32m    263\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/anyio/streams/memory.py:119\u001b[39m, in \u001b[36mMemoryObjectReceiveStream.receive\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._state.waiting_receivers[receive_event] = receiver\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m receive_event.wait()\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28mself\u001b[39m._state.waiting_receivers.pop(receive_event, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/asyncio/locks.py:213\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import smithery\n",
    "import mcp\n",
    "from mcp.client.websocket import websocket_client\n",
    "\n",
    "# Create Smithery URL with server endpoint\n",
    "url = smithery.create_smithery_url(\"wss://server.smithery.ai/@smithery-ai/github/ws\", {\n",
    "  \"githubPersonalAccessToken\": GITHUB_TOKEN\n",
    "}) + SMITHERY_API_KEY\n",
    "\n",
    "async def github_operations():\n",
    "    # Connect to the server using websocket client\n",
    "    async with websocket_client(url) as streams:\n",
    "        async with mcp.ClientSession(*streams) as session:\n",
    "            # List available tools (optional, to see what's available)\n",
    "            tools_result = await session.list_tools()\n",
    "            print(f\"Available tools: {', '.join([t.name for t in tools_result])}\")\n",
    "            \n",
    "            # # Example: Fetch repository info (adjust tool_name and params)\n",
    "            # repo_info = await session.call_tool(\n",
    "            #     \"get_repository\",  # Hypothetical tool name (check actual available tools)\n",
    "            #     {\n",
    "            #         \"owner\": \"TeodorRusKvi\",\n",
    "            #         \"public_repo\": \"Text-Analysis\",\n",
    "            #     }\n",
    "            # )\n",
    "            # print(\"Repository info:\", repo_info)\n",
    "\n",
    "            # Example: Create an issue (adjust tool_name and params)\n",
    "            # new_issue = await session.call_tool(\n",
    "            #     \"create_issue\",\n",
    "            #     {\n",
    "            #         \"owner\": \"your-github-username\",\n",
    "            #         \"repo\": \"your-repo-name\",\n",
    "            #         \"title\": \"Bug: Something is broken\",\n",
    "            #         \"body\": \"Detailed description of the issue...\"\n",
    "            #     }\n",
    "            # )\n",
    "            # print(\"Created issue:\", new_issue)\n",
    "await github_operations() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab01de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[90m[INFO]\u001b[0m HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[90m[INFO]\u001b[0m MCP tool \"server1\"/\"multiply\" received input: {'a': 3, 'b': 5}\n",
      "\u001b[90m[INFO]\u001b[0m MCP tool \"server1\"/\"multiply\" received result (size: 1)\n",
      "\u001b[90m[INFO]\u001b[0m HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[90m[INFO]\u001b[0m MCP tool \"server1\"/\"multiply\" received input: {'a': 8, 'b': 12}\n",
      "\u001b[90m[INFO]\u001b[0m MCP tool \"server1\"/\"multiply\" received result (size: 2)\n",
      "\u001b[90m[INFO]\u001b[0m HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"what's (3 + 5) x 12?\", additional_kwargs={}, response_metadata={}, id='757783ee-0300-4171-b05b-7242857561bd'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_JisHTAVIcVggZjN7M3IwnjhI', 'function': {'arguments': '{\"a\":3,\"b\":5}', 'name': 'add'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 149, 'total_tokens': 167, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_b7faba9ef5', 'id': 'chatcmpl-BLktu6jmSEFyqrOtk8DN1a6gJ88z6', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5ba96186-719c-4450-beab-68dea005c8fe-0', tool_calls=[{'name': 'add', 'args': {'a': 3, 'b': 5}, 'id': 'call_JisHTAVIcVggZjN7M3IwnjhI', 'type': 'tool_call'}], usage_metadata={'input_tokens': 149, 'output_tokens': 18, 'total_tokens': 167, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='8', name='add', id='dc90aac2-0e86-4497-8aab-da7dce1ff153', tool_call_id='call_JisHTAVIcVggZjN7M3IwnjhI'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_AbYVTwoZnrlBOfxZmiIaVORS', 'function': {'arguments': '{\"a\":8,\"b\":12}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 174, 'total_tokens': 192, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_b7faba9ef5', 'id': 'chatcmpl-BLktvzvaCxt9xZ4ucetAG4K2KmFvp', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-fdc9526c-ea50-4025-82fa-7d8ef82d1ab1-0', tool_calls=[{'name': 'multiply', 'args': {'a': 8, 'b': 12}, 'id': 'call_AbYVTwoZnrlBOfxZmiIaVORS', 'type': 'tool_call'}], usage_metadata={'input_tokens': 174, 'output_tokens': 18, 'total_tokens': 192, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='96', name='multiply', id='d9e0b18c-baa6-42da-85f0-84891553d095', tool_call_id='call_AbYVTwoZnrlBOfxZmiIaVORS'),\n",
       "  AIMessage(content='The result of \\\\((3 + 5) \\\\times 12\\\\) is 96.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 199, 'total_tokens': 221, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_b7faba9ef5', 'id': 'chatcmpl-BLktwjF4hOX3t04YnoWHKYvKzItat', 'finish_reason': 'stop', 'logprobs': None}, id='run-6425d356-3d7b-4269-8f73-de526f83375e-0', usage_metadata={'input_tokens': 199, 'output_tokens': 22, 'total_tokens': 221, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "agent_response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69accbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/13/25 08:02:52] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/chat/completions</span>          <a href=\"file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/13/25 08:02:52]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/chat/completions\u001b[0m          \u001b]8;id=412224;file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=505021;file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                                                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> MCP tool <span style=\"color: #008000; text-decoration-color: #008000\">\"server2\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #008000; text-decoration-color: #008000\">\"get-forecast\"</span> received input:           <a href=\"file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">langchain_mcp_tools.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py#234\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">234</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'latitude'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">38.627003</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'longitude'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-90.199402</span><span style=\"font-weight: bold\">}</span>            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                          </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m MCP tool \u001b[32m\"server2\"\u001b[0m\u001b[35m/\u001b[0m\u001b[32m\"get-forecast\"\u001b[0m received input:           \u001b]8;id=956214;file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py\u001b\\\u001b[2mlangchain_mcp_tools.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=279227;file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py#234\u001b\\\u001b[2m234\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'latitude'\u001b[0m: \u001b[1;36m38.627003\u001b[0m, \u001b[32m'longitude'\u001b[0m: \u001b[1;36m-90.199402\u001b[0m\u001b[1m}\u001b[0m            \u001b[2m                          \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/13/25 08:02:53] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> MCP tool <span style=\"color: #008000; text-decoration-color: #008000\">\"server2\"</span><span style=\"color: #800080; text-decoration-color: #800080\">/</span><span style=\"color: #008000; text-decoration-color: #008000\">\"get-forecast\"</span> received result <span style=\"font-weight: bold\">(</span>size:    <a href=\"file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">langchain_mcp_tools.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py#270\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">270</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1149</span><span style=\"font-weight: bold\">)</span>                                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                          </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/13/25 08:02:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m MCP tool \u001b[32m\"server2\"\u001b[0m\u001b[35m/\u001b[0m\u001b[32m\"get-forecast\"\u001b[0m received result \u001b[1m(\u001b[0msize:    \u001b]8;id=37982;file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py\u001b\\\u001b[2mlangchain_mcp_tools.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=659100;file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/langchain_mcp_tools/langchain_mcp_tools.py#270\u001b\\\u001b[2m270\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1;36m1149\u001b[0m\u001b[1m)\u001b[0m                                                       \u001b[2m                          \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[04/13/25 08:02:57] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/chat/completions</span>          <a href=\"file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/httpx/_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[04/13/25 08:02:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/chat/completions\u001b[0m          \u001b]8;id=869680;file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=594180;file:///home/teodorrk/projects/openbridge-mvp/.myvenv/lib/python3.11/site-packages/httpx/_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                                                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='How is the weather in st. louis, MO?', additional_kwargs={}, response_metadata={}, id='7fb8a719-4994-4c02-9eb2-5f0291b7309d'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_AT13lsISd4Pg7tX9cGI6EWk5', 'function': {'arguments': '{\"latitude\":38.627003,\"longitude\":-90.199402}', 'name': 'get-forecast'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 148, 'total_tokens': 175, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_b7faba9ef5', 'id': 'chatcmpl-BLkiZ73kwxLXYhgvyBwEsNLfSQpe9', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2f344dc7-9daa-4180-b1ae-13174e39d5c3-0', tool_calls=[{'name': 'get-forecast', 'args': {'latitude': 38.627003, 'longitude': -90.199402}, 'id': 'call_AT13lsISd4Pg7tX9cGI6EWk5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 148, 'output_tokens': 27, 'total_tokens': 175, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='Forecast for 38.627003, -90.199402:\\n\\nOvernight:\\nTemperature: 49°F\\nWind: 12 mph SE\\nPartly Cloudy\\n---\\nSunday:\\nTemperature: 79°F\\nWind: 13 to 18 mph S\\nSunny\\n---\\nSunday Night:\\nTemperature: 60°F\\nWind: 8 to 14 mph SW\\nPartly Cloudy\\n---\\nMonday:\\nTemperature: 68°F\\nWind: 8 to 13 mph NW\\nMostly Cloudy\\n---\\nMonday Night:\\nTemperature: 46°F\\nWind: 12 mph W\\nPartly Cloudy\\n---\\nTuesday:\\nTemperature: 63°F\\nWind: 12 to 15 mph NW\\nSunny\\n---\\nTuesday Night:\\nTemperature: 42°F\\nWind: 2 to 9 mph W\\nMostly Clear\\n---\\nWednesday:\\nTemperature: 68°F\\nWind: 2 to 6 mph S\\nMostly Sunny\\n---\\nWednesday Night:\\nTemperature: 51°F\\nWind: 9 mph SE\\nChance Showers And Thunderstorms\\n---\\nThursday:\\nTemperature: 74°F\\nWind: 13 mph SE\\nChance Rain Showers then Slight Chance Showers And Thunderstorms\\n---\\nThursday Night:\\nTemperature: 63°F\\nWind: 12 mph S\\nSlight Chance Showers And Thunderstorms then Chance Showers And Thunderstorms\\n---\\nFriday:\\nTemperature: 83°F\\nWind: 10 to 14 mph S\\nChance Showers And Thunderstorms\\n---\\nFriday Night:\\nTemperature: 58°F\\nWind: 6 to 12 mph SW\\nChance Showers And Thunderstorms\\n---\\nSaturday:\\nTemperature: 69°F\\nWind: 8 mph NW\\nChance Showers And Thunderstorms\\n---', name='get-forecast', id='c3d6e2af-0785-439d-9bd8-3b72b1c711c3', tool_call_id='call_AT13lsISd4Pg7tX9cGI6EWk5'),\n",
       "  AIMessage(content='The weather forecast for St. Louis, MO is as follows:\\n\\n- **Overnight**: Partly Cloudy, with a low of 49°F and winds from the southeast at 12 mph.\\n- **Sunday**: Sunny with a high of 79°F and south wind between 13 and 18 mph.\\n- **Sunday Night**: Partly Cloudy, low of 60°F, and southwest winds between 8 and 14 mph.\\n- **Monday**: Mostly Cloudy with a high of 68°F, and northwest winds between 8 and 13 mph.\\n- **Monday Night**: Partly Cloudy, low of 46°F, with west winds at 12 mph.\\n- **Tuesday**: Sunny with a high of 63°F and northwest winds ranging from 12 to 15 mph.\\n- **Tuesday Night**: Mostly Clear, low of 42°F, and west winds from 2 to 9 mph.\\n- **Wednesday**: Mostly Sunny with a high of 68°F and south winds between 2 and 6 mph.\\n- **Wednesday Night**: Chance of Showers and Thunderstorms with a low of 51°F and southeast winds at 9 mph.\\n- **Thursday**: Chance of Rain Showers with a high of 74°F and southeast winds at 13 mph.\\n- **Thursday Night**: Slight Chance of Showers and Thunderstorms, transitioning to Chance Showers and Thunderstorms, with a low of 63°F and south winds at 12 mph.\\n- **Friday**: Chance of Showers and Thunderstorms with a high of 83°F and south wind between 10 and 14 mph.\\n- **Friday Night**: Chance of Showers and Thunderstorms with a low of 58°F and southwest winds from 6 to 12 mph.\\n- **Saturday**: Chance of Showers and Thunderstorms, with a high of 69°F and northwest winds at 8 mph.\\n\\nBe sure to check the local conditions closer to your planned activities!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 417, 'prompt_tokens': 538, 'total_tokens': 955, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_b7faba9ef5', 'id': 'chatcmpl-BLkicI2AEbvKMBvry1ITylSfMQ7rD', 'finish_reason': 'stop', 'logprobs': None}, id='run-0a767496-32f4-480b-b01c-22b8b0c5df1f-0', usage_metadata={'input_tokens': 538, 'output_tokens': 417, 'total_tokens': 955, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_response = await agent.ainvoke({\"messages\": \"How is the weather in st. louis, MO?\"})\n",
    "agent_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "224e1977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenBridge packages not found at: ./node_modules/@oicl\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 140\u001b[39m\n\u001b[32m    137\u001b[39m project_root = \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Default to current directory\u001b[39;00m\n\u001b[32m    138\u001b[39m results = find_openbridge_components(project_root)\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m formatted_output, component_names = \u001b[43mformat_component_names_for_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[38;5;28mprint\u001b[39m(formatted_output)\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Save to file for later use with LLM\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mformat_component_names_for_llm\u001b[39m\u001b[34m(results)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03mFormat the component data for use as LLM context.\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    114\u001b[39m component_names = [comp[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m comp \u001b[38;5;129;01min\u001b[39;00m results[\u001b[33m\"\u001b[39m\u001b[33mcomponents\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m component_names = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcomponent_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Remove duplicates\u001b[39;00m\n\u001b[32m    116\u001b[39m component_names.sort()\n\u001b[32m    118\u001b[39m output = \u001b[33m\"\u001b[39m\u001b[33m# OpenBridge Components\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: 'dict' object is not callable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "def find_openbridge_components(project_root):\n",
    "    \"\"\"\n",
    "    Find OpenBridge components in the node_modules directory.\n",
    "    \n",
    "    Args:\n",
    "        project_root: Path to the project root directory\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing component information from different sources\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"components\": [],\n",
    "        \"package_info\": []\n",
    "    }\n",
    "    \n",
    "    # Path to OpenBridge packages in node_modules\n",
    "    openbridge_base_path = os.path.join(project_root, 'node_modules', '@oicl')\n",
    "    \n",
    "    # Check if the package is installed\n",
    "    if not os.path.exists(openbridge_base_path):\n",
    "        print(f\"OpenBridge packages not found at: {openbridge_base_path}\")\n",
    "        \n",
    "        # Try to get at least some information from package-lock.json\n",
    "        package_lock_path = os.path.join(project_root, 'package-lock.json')\n",
    "        if os.path.exists(package_lock_path):\n",
    "            with open(package_lock_path, 'r') as f:\n",
    "                package_lock = json.load(f)\n",
    "                \n",
    "                # Extract package information\n",
    "                for pkg_name, pkg_info in package_lock.get('packages', {}).items():\n",
    "                    if '@oicl/openbridge' in pkg_name:\n",
    "                        results[\"package_info\"].append({\n",
    "                            \"package\": pkg_name,\n",
    "                            \"version\": pkg_info.get(\"version\", \"unknown\")\n",
    "                        })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Look for component files\n",
    "    component_paths = []\n",
    "    component_paths += glob.glob(os.path.join(openbridge_base_path, 'openbridge-webcomponents', '**', '*.js'), recursive=True)\n",
    "    component_paths += glob.glob(os.path.join(openbridge_base_path, 'openbridge-webcomponents', '**', '*.ts'), recursive=True)\n",
    "    component_paths += glob.glob(os.path.join(openbridge_base_path, 'openbridge-webcomponents-react', '**', '*.js'), recursive=True)\n",
    "    component_paths += glob.glob(os.path.join(openbridge_base_path, 'openbridge-webcomponents-react', '**', '*.ts'), recursive=True)\n",
    "    \n",
    "    # Process each file\n",
    "    for file_path in component_paths:\n",
    "        if 'node_modules' in file_path and (\n",
    "            '/components/' in file_path or \n",
    "            '/src/' in file_path or \n",
    "            '/dist/' in file_path\n",
    "        ):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    \n",
    "                    # Look for component definitions\n",
    "                    custom_elements = re.findall(r'customElements\\.define\\([\\'\"]([^\\'\"]+-[^\\'\"]+)[\\'\"]', content)\n",
    "                    class_defs = re.findall(r'class\\s+(\\w+)\\s+extends\\s+(HTMLElement|LitElement|Component)', content)\n",
    "                    exports = re.findall(r'export\\s+(const|class)\\s+(\\w+)', content)\n",
    "                    \n",
    "                    # Extract component names\n",
    "                    for elem in custom_elements:\n",
    "                        results[\"components\"].append({\n",
    "                            \"name\": elem,\n",
    "                            \"type\": \"web-component\",\n",
    "                            \"source\": os.path.relpath(file_path, project_root)\n",
    "                        })\n",
    "                    \n",
    "                    for class_name, _ in class_defs:\n",
    "                        results[\"components\"].append({\n",
    "                            \"name\": class_name,\n",
    "                            \"type\": \"component-class\",\n",
    "                            \"source\": os.path.relpath(file_path, project_root)\n",
    "                        })\n",
    "                    \n",
    "                    for _, export_name in exports:\n",
    "                        if export_name not in [comp[\"name\"] for comp in results[\"components\"]]:\n",
    "                            results[\"components\"].append({\n",
    "                                \"name\": export_name,\n",
    "                                \"type\": \"export\",\n",
    "                                \"source\": os.path.relpath(file_path, project_root)\n",
    "                            })\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {e}\")\n",
    "    \n",
    "    # Check for package.json files to get metadata\n",
    "    for package_name in ['openbridge-webcomponents', 'openbridge-webcomponents-react']:\n",
    "        package_json_path = os.path.join(openbridge_base_path, package_name, 'package.json')\n",
    "        if os.path.exists(package_json_path):\n",
    "            try:\n",
    "                with open(package_json_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    results[\"package_info\"].append({\n",
    "                        \"name\": data.get(\"name\", package_name),\n",
    "                        \"version\": data.get(\"version\", \"unknown\"),\n",
    "                        \"description\": data.get(\"description\", \"\")\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading package.json {package_json_path}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def format_component_names_for_llm(results):\n",
    "    \"\"\"\n",
    "    Format the component data for use as LLM context.\n",
    "    \"\"\"\n",
    "    component_names = [comp[\"name\"] for comp in results[\"components\"]]\n",
    "    component_names = list(set(component_names))  # Remove duplicates\n",
    "    component_names.sort()\n",
    "    \n",
    "    output = \"# OpenBridge Components\\n\\n\"\n",
    "    \n",
    "    # Add package info\n",
    "    if results[\"package_info\"]:\n",
    "        output += \"## Package Information\\n\"\n",
    "        for pkg in results[\"package_info\"]:\n",
    "            output += f\"- {pkg.get('name', 'Unknown')}: v{pkg.get('version', 'unknown')}\\n\"\n",
    "            if pkg.get('description'):\n",
    "                output += f\"  Description: {pkg['description']}\\n\"\n",
    "        output += \"\\n\"\n",
    "    \n",
    "    # Add component list\n",
    "    output += \"## Component Names\\n\"\n",
    "    for name in component_names:\n",
    "        output += f\"- {name}\\n\"\n",
    "    \n",
    "    return output, component_names\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_root = \".\"  # Default to current directory\n",
    "    results = find_openbridge_components(project_root)\n",
    "    \n",
    "    formatted_output, component_names = format_component_names_for_llm(results)\n",
    "    print(formatted_output)\n",
    "    \n",
    "    # Save to file for later use with LLM\n",
    "    with open(\"openbridge_components.txt\", \"w\") as f:\n",
    "        f.write(formatted_output)\n",
    "    \n",
    "    print(f\"\\nFound {len(component_names)} unique component names.\")\n",
    "    print(f\"Component list saved to openbridge_components.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47094e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep search found 0 potential files\n"
     ]
    }
   ],
   "source": [
    "# Add this to the find_openbridge_components function\n",
    "def deep_search(project_root):\n",
    "    results = []\n",
    "    for root, dirs, files in os.walk(os.path.join(project_root, 'node_modules')):\n",
    "        print(f\"Searching in {root}\")\n",
    "        for file in files:\n",
    "            if 'openbridge' in root.lower() and file.endswith(('.js', '.ts')):\n",
    "                results.append(os.path.join(root, file))\n",
    "    return results\n",
    "    \n",
    "# Use this if the normal search fails\n",
    "#if not results[\"components\"]:\n",
    "deep_files = deep_search(project_root)\n",
    "print(f\"Deep search found {len(deep_files)} potential files\")\n",
    "# Process these files similar to the main function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
